---
template: post
title: Explaining adversarial vulnerabiity with a data sparisty hypothesis
date: 2022-01-29T03:20:00.000Z
journaltypes: Journal Paper
journal: "Neurocomputing, 2022, Neurocomputing, 2022, doi: 10.1016/j.neucom.2022.01.062"
pubmed: nil
url: https://www.sciencedirect.com/science/article/abs/pii/S0925231222000819
impactfactor: "5.719"
dateofacceptance: 2022-01-15T03:20:00.000Z
description: Despite many proposed algorithms to provide robustness to deep
  learning (DL) models, DL models remain susceptible to adversarial attacks. We
  hypothesize that the adversarial vulnerability of DL models stems from two
  factors. The first factor is data sparsity which is that in the high
  dimensional input data space, there exist large regions outside the support of
  the data distribution. The second factor is the existence of many redundant
  parameters in the DL models. Owing to these factors, different models are able
  to come up with different decision boundaries with comparably high prediction
  accuracy.
tags:
  - Paknezhad M
  - Cuong PN
  - Winarto AA
  - Cheong A
  - Beh CY
  - Wu J
categories:
  - Computer Vision and Pattern Discovery for Bioimages
---
<!--StartFragment-->

Despite many proposed algorithms to provide robustness to deep learning (DL) models, DL models remain susceptible to [adversarial attacks](https://www.sciencedirect.com/topics/computer-science/adversarial-machine-learning "Learn more about adversarial attacks from ScienceDirect's AI-generated Topic Pages"). We hypothesize that the adversarial vulnerability of DL models stems from two factors. The first factor is data [sparsity](https://www.sciencedirect.com/topics/computer-science/sparsity "Learn more about sparsity from ScienceDirect's AI-generated Topic Pages") which is that in the high dimensional input data space, there exist large regions outside the support of the data distribution. The second factor is the existence of many redundant parameters in the DL models. Owing to these factors, different models are able to come up with different decision boundaries with comparably high prediction accuracy. The appearance of the decision boundaries in the space outside the support of the data distribution does not affect the prediction accuracy of the model. However, it makes an important difference in the adversarial robustness of the model. We hypothesize that the ideal decision boundary is as far as possible from the support of the data distribution. In this paper, we develop a training framework to observe if DL models are able to learn such a decision boundary spanning the space around the class distributions further from the data points themselves. Semi-supervised learning was deployed during training by leveraging [unlabeled data](https://www.sciencedirect.com/topics/computer-science/unlabeled-data "Learn more about unlabeled data from ScienceDirect's AI-generated Topic Pages") generated in the space outside the support of the data distribution. We measured adversarial robustness of the models trained using this training framework against well-known [adversarial attacks](https://www.sciencedirect.com/topics/computer-science/adversarial-machine-learning "Learn more about adversarial attacks from ScienceDirect's AI-generated Topic Pages") and by using robustness metrics. We found that models trained using our framework, as well as other [regularization methods](https://www.sciencedirect.com/topics/computer-science/regularization-method "Learn more about regularization methods from ScienceDirect's AI-generated Topic Pages") and adversarial training support our hypothesis of data [sparsity](https://www.sciencedirect.com/topics/computer-science/sparsity "Learn more about sparsity from ScienceDirect's AI-generated Topic Pages") and that models trained with these methods learn to have decision boundaries more similar to the aforementioned ideal decision boundary. We show that the [unlabeled data](https://www.sciencedirect.com/topics/computer-science/unlabeled-data "Learn more about unlabeled data from ScienceDirect's AI-generated Topic Pages") generated by noise in our framework is almost as effective on adversarial robustness as unlabeled data sourced from existing datasets or generated by synthesis algorithms. The code for our training framework is available online.

<!--EndFragment-->